{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import scipy.stats as spy\n",
    "from numba import njit, prange\n",
    "\n",
    "sns.set()      \n",
    "sns.set_context(\"talk\")\n",
    "\n",
    "def fact_arr(n):\n",
    "    # takes the factorial of an array \"n\" of values\n",
    "    fact_n=np.zeros(n.shape[0])\n",
    "    for i in range(n.shape[0]):\n",
    "        fact_n[i]=np.math.factorial(n[i])\n",
    "    return fact_n\n",
    "def p_Poiss(mu,n):\n",
    "    # returns the probability of n events for a mean value of mu in a Poisson process\n",
    "    return mu**n*np.exp(-mu)/fact_arr(n)\n",
    "def port_prob(mu,d_arr,sort,eta):\n",
    "    # using the system properties (mean photon number \"mu\", dark counts array \"d_arr\", polarization sorting matrix \"sort\",\n",
    "    # channel loss \"eta\"), generates a lookup table of probabilties of outputs (columns) for different inputs (rows).  \n",
    "    # 5th row is vacuum state.\n",
    "    n_pol_states = sort.shape[0]\n",
    "    n_detectors = len(d_arr)\n",
    "    p_click_mat = np.zeros([n_pol_states+1,n_detectors])\n",
    "    mu_mat = sort@eta*mu\n",
    "    for polar in range(n_pol_states):\n",
    "        for port in range(n_detectors):\n",
    "            p_click_mat[polar,port] = 1-(p_Poiss(mu_mat[polar,port],np.array([0.]))*(1-d_arr[port]))\n",
    "    p_click_mat[n_pol_states,:] = np.transpose(d_arr)\n",
    "    return p_click_mat\n",
    "\n",
    "def nlik_prob(p_click_mat,p_send_arr):\n",
    "    # returns the weighted average detection probability for the different detectors using the input-output lookup\n",
    "    # table \"p_click_mat\" and the frequencies of the different inputs \"p_send_arr\"\n",
    "    n_detectors = p_click_mat.shape[1]\n",
    "    #nlik_arr = 1-np.prod(1-p_click_mat,0)\n",
    "    nlik_arr=np.ones(n_detectors)\n",
    "    for port in range(n_detectors):\n",
    "        for polar in range(len(p_send_arr)):\n",
    "            nlik_arr[port] = nlik_arr[port]*(1-p_send_arr[polar]*p_click_mat[polar,port])\n",
    "            # first finds the approximate probability of getting no clicks\n",
    "    return 1-nlik_arr\n",
    "def DataSifter(Test,Stored,d=2):\n",
    "    \"\"\"Takes Alice's (Test) and Bob's (Stored) synchronized data strings, and sifts them.  Yields Alice and Bob's nearly \n",
    "    identical sifted arrays (dark counts prevent this from being perfect), and the boolean mask that produced them.\"\"\"\n",
    "    # d is the dimension of the basis, but implementation of d dimensional synchronization is incomplete\n",
    "    Basis_A=np.array([np.sum(Test[:d,:],0),np.sum(Test[d:,:],0)])\n",
    "    # 1 is H/V, 2 is R/L\n",
    "    Basis_B=np.array([np.sum(Stored[:d,:],0),np.sum(Stored[d:,:],0)])\n",
    "    #overlap=(1-Stored[0,:]*Stored[1,:])*(1-Stored[2,:]*Stored[3,:])*(1-Basis_B[0,:]*Basis_B[1,:])\n",
    "    overlap = (np.sum(Stored,0)==1)*1  # exactly one detection\n",
    "    Basis_B[0,:]=Basis_B[0,:]*overlap\n",
    "    Basis_B[1,:]=Basis_B[1,:]*overlap\n",
    "    sift=sum(Basis_A*Basis_B)\n",
    "    sifter=np.zeros(Test.shape)\n",
    "    for i in range(d*2):\n",
    "        sifter[i,:]=sift\n",
    "    SiftedTest=np.reshape(Test[sifter==np.ones(sifter.shape)],[d*2,int(sum(sift))])\n",
    "    SiftedStored=np.reshape(Stored[sifter==np.ones(sifter.shape)],[d*2,int(sum(sift))])\n",
    "    return SiftedTest, SiftedStored, sift\n",
    "def Syncing_metric(Test,Stored,not_Stored,N_prior,p_click_mat,nlik_arr):\n",
    "    \"\"\"\"Computes the synchronization metric for the Syncing_try function\"\"\"\n",
    "    #Test5 = np.zeros([5,Test.shape[1]])\n",
    "    #Test5[0:4,:] = Test\n",
    "    #Test5[4,:] = (np.sum(Test,0)==0)*1\n",
    "    count_mat = Test@(np.transpose(Stored))\n",
    "    no_count_mat = Test@(np.transpose(not_Stored))\n",
    "    \n",
    "    lik = np.log(1/N_prior)\n",
    "    nlik=np.log((N_prior-1)/N_prior)\n",
    "    for i in range(Test.shape[0]):\n",
    "        for j in range(Stored.shape[0]):\n",
    "            lik = lik+np.log(p_click_mat[i,j])*count_mat[i,j]+np.log(1-p_click_mat[i,j])*no_count_mat[i,j]\n",
    "            nlik = nlik+np.log(nlik_arr[j])*count_mat[i,j]+np.log(1-nlik_arr[j])*no_count_mat[i,j]\n",
    "    \n",
    "    if ((lik-nlik)<700) & ((lik-nlik)>-700):\n",
    "        return np.exp((lik-nlik))/(np.exp(lik-nlik)+np.exp(1))\n",
    "    elif (lik-nlik)>700:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def Syncing_try(TestPatch,StoredPatch,sRange,recovered_length,mu_sig,mu_dec,d_arr,sort,eta,p_send_arr,Ntries=0,Mask=[-1]):\n",
    "    \"\"\"Simplified, nearly deprecated, slow, and mathematically tenuous way to the synchronization probability.  Uses a for \n",
    "    loop to try different synchronization points.  Consider using Syncing_fft.\"\"\"\n",
    "    n_pol_states = sort.shape[0]\n",
    "    n_detectors = len(d_arr)\n",
    "    not_StoredPatch = 1 - StoredPatch\n",
    "    if Mask[0] == -1:\n",
    "        Mask=np.ones([TestPatch.shape[1]])\n",
    "    FullMask = np.zeros(TestPatch.shape)\n",
    "    for i in range(TestPatch.shape[0]):\n",
    "        FullMask[i,:] = Mask  \n",
    "        # one might want to apply a boolean mask to choose which points to use for synchronization\n",
    "        # but I longer find this a useful feature since I no longer burn qubits for synchronization\n",
    "    TestPatch = TestPatch * FullMask\n",
    "    N=Ntries # number of points to try, basically deprecated\n",
    "    if Ntries==0:\n",
    "        N=StoredPatch.shape[1]-TestPatch.shape[1]+1\n",
    "    metric=np.zeros(recovered_length)\n",
    "    p_click_mat_sig = port_prob(mu_sig,d_arr,sort,eta)\n",
    "    p_click_mat_dec = port_prob(mu_dec,d_arr,sort,eta)\n",
    "    # full 9 input state lookup table (4 signals, 4 decoys, 1 vacuum)\n",
    "    p_click_mat = np.zeros([TestPatch.shape[0],StoredPatch.shape[0]]) \n",
    "    p_click_mat[:n_pol_states,:] = p_click_mat_sig[:n_pol_states,:]\n",
    "    p_click_mat[n_pol_states:,:] = p_click_mat_dec\n",
    "    nlik_arr = nlik_prob(p_click_mat,p_send_arr)\n",
    "    nlik_mat = np.zeros(p_click_mat.shape)\n",
    "    for i in range(nlik_mat.shape[0]):\n",
    "        nlik_mat[i,:] = nlik_arr\n",
    "        \n",
    "    for i in range(N):\n",
    "        StoredPatch_short=StoredPatch[:,i:i+TestPatch.shape[1]]\n",
    "        not_StoredPatch_short=not_StoredPatch[:,i:i+TestPatch.shape[1]]\n",
    "        metric[i+sRange[0]]=Syncing_metric(TestPatch,StoredPatch_short,not_StoredPatch_short,N,p_click_mat,nlik_arr)\n",
    "    return metric\n",
    "\n",
    "\n",
    "def Syncing_fft(TestPatch,StoredPatch,sRange,recovered_length,\\\n",
    "                mu_sig,mu_dec,d_arr,sort,eta,size,num_blocks,p_send_arr,Ntries=0,Mask=[-1],normalize=1,TestStart=0):\n",
    "    \"\"\"Uses a a fast fourier transform to count the numbers of different events along with a probability lookup table to\n",
    "    compute the synchronization probability using TestPatch at different time offsets of Stored.  Normalize = 1 is \n",
    "    preferred (and mathematically justifiable) method.\"\"\"\n",
    "    n_pol_states = sort.shape[0]\n",
    "    n_detectors = len(d_arr)\n",
    "    if Mask[0] == -1:\n",
    "        Mask=np.ones(TestPatch.shape[1])\n",
    "    TestMask = np.zeros(TestPatch.shape)\n",
    "    #StoredPatch = Stored[:,sRange[0]:(sRange[1]+Ntries)]\n",
    "    for i in range(TestPatch.shape[0]):\n",
    "        TestMask[i,:] = Mask\n",
    "        # one might want to apply a boolean mask to choose which points to use for synchronization\n",
    "        # but I longer find this a useful feature since I no longer burn qubits for synchronization\n",
    "    TestPatch = TestPatch*TestMask\n",
    "    #BlankPatch = 1-np.sum(TestPatch,0)\n",
    "    padded_length = next_power(StoredPatch.shape[1],2) # ffts want things padded with zeros to the next power of 2\n",
    "    BlankPatch = np.zeros([2,padded_length])\n",
    "    #BlankPatch[0,:] = np.ones(padded_length)\n",
    "    for i in range(padded_length): # making note of when something was sent and when nothing was sent across whole data\n",
    "        BlankPatch[0,i] = ((((i+TestStart)%size)>=0) & (((i+TestStart)%size)<num_blocks))\n",
    "        BlankPatch[1,i] = (((i+TestStart)%size)>=num_blocks)\n",
    "    TestPatch_padded = np.zeros([TestPatch.shape[0],padded_length])\n",
    "    TestPatch_padded[:,:TestPatch.shape[1]] = TestPatch\n",
    "    StoredPatch_padded = np.zeros([StoredPatch.shape[0],padded_length])\n",
    "    StoredPatch_padded[:,:StoredPatch.shape[1]] = StoredPatch\n",
    "    NotStoredPatch_padded = np.zeros([StoredPatch.shape[0],padded_length])\n",
    "    NotStoredPatch_padded[:,:StoredPatch.shape[1]] = (1-StoredPatch)\n",
    "    #BlankPatch_padded = np.zeros([padded_length])\n",
    "    #BlankPatch_padded[:BlankPatch.shape[0]] = BlankPatch\n",
    "    \n",
    "    if Ntries==0: # number of points to try, basically deprecated\n",
    "        Ntries=StoredPatch.shape[1]-TestPatch.shape[1]+1\n",
    "    N_prior = Ntries # basic prior is one over the number of candidates\n",
    "    #print(N_prior)\n",
    "    #N_prior=2\n",
    "    metric=np.zeros(recovered_length-TestPatch.shape[1]+1)\n",
    "    p_click_mat_sig = port_prob(mu_sig,d_arr,sort,eta)\n",
    "    p_click_mat_dec = port_prob(mu_dec,d_arr,sort,eta)\n",
    "    p_click_mat = np.zeros([TestPatch.shape[0],StoredPatch.shape[0]])\n",
    "    # full 9 input state lookup table (4 signals, 4 decoys, 1 vacuum)\n",
    "    p_click_mat[:n_pol_states,:] = p_click_mat_sig[:n_pol_states,:]\n",
    "    p_click_mat[n_pol_states:,:] = p_click_mat_dec\n",
    "    nlik_arr = nlik_prob(p_click_mat,p_send_arr) # uninformed detection probabilities\n",
    "    nlik_mat = np.zeros(p_click_mat.shape) # nice to have it in a matrix form\n",
    "    for i in range(nlik_mat.shape[0]):\n",
    "        nlik_mat[i,:] = nlik_arr\n",
    "    # counting up the different event combinations using ffts\n",
    "    count_mat = np.zeros([padded_length,TestPatch.shape[0],StoredPatch.shape[0]])\n",
    "    no_count_mat = np.zeros([padded_length,TestPatch.shape[0],StoredPatch.shape[0]])\n",
    "    blank_mat = np.zeros([padded_length,2,StoredPatch.shape[0]])\n",
    "    for j in range(StoredPatch.shape[0]):\n",
    "        for i in range(TestPatch.shape[0]):\n",
    "            count_mat[:,i,j] = np.round(np.real(np.fft.irfft(np.conj(np.fft.rfft(TestPatch_padded[i,:]))\\\n",
    "                                                           *np.fft.rfft(StoredPatch_padded[j,:]))))\n",
    "            #no_count_mat[:,i,j] = np.round(np.real(np.fft.irfft(np.conj(np.fft.rfft(TestPatch_padded[i,:]))\\\n",
    "            #                                               *np.fft.rfft(NotStoredPatch_padded[j,:]))))\n",
    "        blank_mat[:,0,j] = np.round(np.real(np.fft.irfft(np.conj(np.fft.rfft(BlankPatch[0,:]))\\\n",
    "                                                           *np.fft.rfft(StoredPatch_padded[j,:]))))\n",
    "        blank_mat[:,1,j] = np.round(np.real(np.fft.irfft(np.conj(np.fft.rfft(BlankPatch[1,:]))\\\n",
    "                                                           *np.fft.rfft(StoredPatch_padded[j,:]))))\n",
    "    no_count_mat = np.zeros(count_mat.shape)\n",
    "    no_blank_mat = np.zeros(blank_mat.shape)\n",
    "    num_sent = np.sum(TestPatch_padded,1)\n",
    "    blank_sent = np.sum(BlankPatch[:,:StoredPatch.shape[1]],1)\n",
    "    for g in range(count_mat.shape[2]):\n",
    "        no_count_mat[:,:,g] = num_sent\n",
    "        no_blank_mat[:,:,g] = blank_sent\n",
    "    no_count_mat = no_count_mat-count_mat\n",
    "    no_blank_mat = no_blank_mat-blank_mat\n",
    "    #print(blank_mat[:30,:,:])\n",
    "    if normalize==0: # not assuming a unique sync point, but the math may also be fishy\n",
    "        for k in range(Ntries):\n",
    "            lik = 0#np.log(1/N_prior)\n",
    "            nlik=np.log((N_prior-1)/N_prior)\n",
    "            lik = lik+np.sum(np.sum(np.log(p_click_mat)*count_mat[k,:,:]\\\n",
    "                +np.log(1-p_click_mat)*no_count_mat[k,:,:]))\n",
    "            nlik = nlik+np.sum(np.sum(np.log(nlik_mat)*count_mat[k,:,:]\\\n",
    "                +np.log(1-nlik_mat)*no_count_mat[k,:,:]))\n",
    "            \n",
    "            if ((lik-nlik)<700) & ((lik-nlik)>-700):\n",
    "                metric[sRange[0]+k] = np.exp((lik-nlik))/(np.exp(lik-nlik)+1)\n",
    "            elif (lik-nlik)>700:\n",
    "                metric[sRange[0]+k] = 1\n",
    "            else:\n",
    "                metric[sRange[0]+k] = 0\n",
    "    whichpoint = 1\n",
    "    if normalize==1: # assuming unique sync point, math is good\n",
    "        lik = np.zeros(Ntries)\n",
    "        nlik = np.zeros(Ntries)\n",
    "        unlik = np.zeros(Ntries)\n",
    "        quo = np.zeros(Ntries)\n",
    "        for k in range(Ntries):\n",
    "            # these are *basically* the 3 terms in the expression at the end of the theory section\n",
    "            lik[k] = np.sum(np.sum(np.log(p_click_mat)*count_mat[k,:,:]\\\n",
    "                +np.log(1-p_click_mat)*no_count_mat[k,:,:]))\n",
    "            nlik[k] = np.sum(np.sum(np.log(nlik_mat)*count_mat[k,:,:]\\\n",
    "                +np.log(1-nlik_mat)*no_count_mat[k,:,:]))\n",
    "            #print(lik[k])#,nlik[k])\n",
    "            unlik[k] = np.sum(np.log(nlik_mat)*blank_mat[k,0,:]+np.log(1-nlik_mat)*no_blank_mat[k,0,:]\\\n",
    "                +np.log(p_click_mat[-1,:])*blank_mat[k,1,:]+np.log(1-p_click_mat[-1,:])*no_blank_mat[k,1,:])\n",
    "        # play some programming games to avoid underflow/overflow issues\n",
    "        quodif = lik -nlik +unlik\n",
    "        ind = np.where(quodif == np.max(quodif))[0][0]\n",
    "        if ind==0:\n",
    "            whichpoint = 1\n",
    "        elif ind==(Ntries-1):\n",
    "            whichpoint = 0\n",
    "        else:\n",
    "            whichpoint=(quodif[ind-1]<quodif[ind+1]) # which neighboring bin is being straddled, we want that too\n",
    "        for k in range(Ntries):\n",
    "            if (quodif[k]-quodif[ind])<-700:\n",
    "                quo[k] = 0\n",
    "            else:\n",
    "                quo[k] = np.exp(quodif[k]-quodif[ind])\n",
    "        metric[sRange[0]:sRange[0]+Ntries] = quo/np.sum(quo)\n",
    "        #metric_data_map = np.memmap('metric_data_mem.npy', dtype='float64',mode = 'w+', shape=(Ntries))\n",
    "        #metric_data_map[:] = metric[sRange[0]:sRange[0]+Ntries].T\n",
    "        #del metric_data_map\n",
    "        #for k in range(Ntries):\n",
    "            #print(np.log(metric[sRange[0]+k]))\n",
    "        #print(np.sum(metric))\n",
    "        print(np.sum(quo))\n",
    "    return metric, whichpoint\n",
    "\n",
    "def next_power(size_old,base): # find the next power of 2, useful for padding arrays before the ffts\n",
    "    size_new = 1\n",
    "    while size_old>size_new:\n",
    "        size_new *= base\n",
    "    return size_new\n",
    "def sync_pt(TestStart,TestPatch,StoredPatch,sRange,recovered_length,\\\n",
    "            mu_sig,mu_dec,d_arr,sort,eta,size,num_blocks,p_send_arr,method='fft',Ntries=0,Mask=[-1],normalize=0):\n",
    "    # this function is mostly error handing and displaying results from the syncing methods\n",
    "    if method == 'fft':\n",
    "        metric, whichpoint = Syncing_fft(TestPatch,StoredPatch,sRange,recovered_length,\\\n",
    "                             mu_sig,mu_dec,d_arr,sort,eta,size,num_blocks,p_send_arr,Ntries,Mask,normalize,TestStart)\n",
    "    if method == 'shift':\n",
    "        whichpoint = 1\n",
    "        metric = Syncing_try(TestPatch,StoredPatch,sRange,recovered_length,\\\n",
    "                             mu_sig,mu_dec,d_arr,sort,eta,p_send_arr,Ntries=0,Mask=[-1])\n",
    "    sync_arr = np.where(metric==np.max(metric))[0]\n",
    "    sync_point = sync_arr[int(np.floor(np.random.uniform(0,1)*sync_arr.shape[0]))]-TestStart\n",
    "    num_sync = sync_arr.shape[0]\n",
    "    #num_sync = np.sum(metric>.5)\n",
    "    two_points = 0\n",
    "    #print(metric)\n",
    "    if num_sync>1:\n",
    "        print('Sync point not unique:')\n",
    "    if num_sync==2:\n",
    "        print('2 viable points')\n",
    "        two_points = 1\n",
    "        #print(np.where(metric==np.max(metric))[0][1]-TestStart)\n",
    "    if num_sync>2:\n",
    "        print('multiple viable points')\n",
    "        print(num_sync)\n",
    "    if np.max(metric)<.5:\n",
    "        print('Correlation is weaker than .5')\n",
    "    return sync_point, np.max(metric), whichpoint\n",
    "def make_bin_array(arr,nbits): # turns a 1d array into nbits rows 2d matrix where the rows are the binary digits\n",
    "    new_arr=np.zeros([nbits,len(arr)])\n",
    "    for j in range(nbits):\n",
    "        new_arr[j,:]=(arr//(2**j))%2\n",
    "    final_arr=np.zeros([nbits+4,len(arr)])\n",
    "    final_arr[4:,:]=new_arr\n",
    "    return final_arr\n",
    "def port_prob_parallel(mu,d_arr,sort,eta): \n",
    "    # wanted to parallelize the port_prob function for use with many different mpns\n",
    "    n_hits = len(mu)\n",
    "    #print(n_hits)\n",
    "    n_pol_states = sort.shape[0]\n",
    "    n_detectors = len(d_arr)\n",
    "    p_click_mat = np.zeros([n_pol_states+1,n_detectors,n_hits])\n",
    "    split = (sort@eta)\n",
    "    mu_mat = np.zeros([n_pol_states,n_detectors,n_hits])\n",
    "    d_mat = np.zeros([n_pol_states,n_detectors,n_hits])\n",
    "    for polar in range(n_pol_states):\n",
    "        for port in range(n_detectors):\n",
    "            mu_mat[polar,port,:] = mu*split[polar,port]\n",
    "            d_mat[polar,port,:] = d_arr[port]*np.ones(n_hits)\n",
    "    p_click_mat[:n_pol_states,:,:] = 1-((spy.poisson.pmf(0,mu_mat))*(1-d_mat))\n",
    "    p_click_mat[n_pol_states,:,:] = d_mat[0,:,:]\n",
    "    return p_click_mat\n",
    "def FakeData_R(Test,mpn_arr,mu_sig,mu_dec,d_arr,sort,eta):\n",
    "    # parallelized measurement simulation for making realistic fake data\n",
    "    n_pol_states = sort.shape[0]\n",
    "    n_detectors = len(d_arr)\n",
    "    n_hits = len(mpn_arr)\n",
    "    p_click_mat = np.zeros([Test.shape[0],n_detectors,n_hits])\n",
    "    Test_arr = np.zeros([Test.shape[0],n_detectors,n_hits])\n",
    "    #print(2)\n",
    "    for j in range(n_detectors):\n",
    "        Test_arr[:,j,:] = Test\n",
    "    #print(3)\n",
    "    p_click_mat_sig = port_prob_parallel(mu_sig*mpn_arr,d_arr,sort,eta)\n",
    "    p_click_mat_dec = port_prob_parallel(mu_dec*mpn_arr,d_arr,sort,eta)\n",
    "    p_click_mat[:n_pol_states,:,:] = p_click_mat_sig[:-1,:]\n",
    "    p_click_mat[n_pol_states:,:,:] = p_click_mat_dec\n",
    "    #print(4)\n",
    "    rand=np.random.uniform(0,1,[n_detectors,Test.shape[1]])\n",
    "    Stored = (np.sum(Test_arr*p_click_mat,0)>rand)\n",
    "    #Stored = ((np.transpose(p_click_mat)@Test)>rand)\n",
    "    #print(5)\n",
    "    return Stored*1\n",
    "def Condense_data(FullData,det):\n",
    "    # For faking data storage\n",
    "    # In the experiment we only record timetags of detections.  This throws out the non-detection events from the fake \n",
    "    # data, so that we can show we can recover them again, just as we would for real data\n",
    "    nonempty = np.zeros(FullData.shape[1])\n",
    "    for i in range(len(det)):\n",
    "        nonempty=nonempty+np.array([FullData[det[i],:]])  # changed to isolate H detections\n",
    "    CondMask=(nonempty>0)\n",
    "    FullMask=np.zeros(FullData.shape)\n",
    "    for i in range(FullData.shape[0]):\n",
    "        FullMask[i,:]=CondMask\n",
    "    CondData=np.reshape(FullData[FullMask==np.ones(FullMask.shape)],[FullData.shape[0],int(sum(sum(CondMask)))])\n",
    "    return CondData\n",
    "def read_time_tag(TimeData):\n",
    "    # turns my N x nbits time array back from binary to decimal\n",
    "    Time_dec=np.zeros(int(TimeData.size/TimeData.shape[0]))\n",
    "    if len(Time_dec)>1:\n",
    "        for i in range(TimeData.shape[0]):\n",
    "            Time_dec=Time_dec+TimeData[i,:]*2**i\n",
    "    else:\n",
    "        for i in range(TimeData.shape[0]):\n",
    "            Time_dec=Time_dec+TimeData[i]*2**i\n",
    "    return Time_dec\n",
    "def ExpandData_sep(TimeData,det):  # det is which detector\n",
    "    # Looking at the gaps between the recorded time tags, fill in all the implied no-detection events\n",
    "    TimeDiff=np.diff(TimeData)\n",
    "    Gaps=np.zeros([1,TimeDiff.shape[1]])\n",
    "    for i in range(TimeData.shape[0]):\n",
    "        Gaps=Gaps+TimeDiff[i,:]*2**i\n",
    "    del TimeDiff\n",
    "    Gaps=np.array(np.mod(Gaps,int(2**TimeData.shape[0])))\n",
    "    ExpData_r=np.zeros(int(np.sum(Gaps[0]))+1)\n",
    "    for i in range(TimeData.shape[1]-1):\n",
    "        ExpData_r[int(np.sum(Gaps[0][:(i+1)])-Gaps[0][i])]=1\n",
    "    ExpData_r[int(np.sum(Gaps))]=1\n",
    "    return ExpData_r\n",
    "def Make_Gaps(TimeData_H):\n",
    "    # take an N long array of time tags and makes an N-1 long array of gaps\n",
    "    # used with gap_prob to identify signal regions of the data\n",
    "    rollover=2**TimeData_H.shape[0]\n",
    "    TimeDiff=np.diff(TimeData_H)\n",
    "    Gaps=np.zeros([1,TimeDiff.shape[1]])\n",
    "    for i in range(TimeData_H.shape[0]):\n",
    "        Gaps=Gaps+TimeDiff[i,:]*2**i\n",
    "    Gaps=np.array(np.mod(Gaps,rollover))\n",
    "    Gaps=Gaps[0]\n",
    "    return Gaps\n",
    "def Join_columns(*args): # this seems to join rows...\n",
    "    # after all the signal regions are recovered for the 4 channels, this slaps them together into one rectified array\n",
    "    #lengths=args.size\n",
    "    args=args[0]\n",
    "    lengths=np.zeros([len(args)])\n",
    "    for i in range(len(args)):\n",
    "        lengths[i]=len(args[i])\n",
    "    #print(max(lengths))\n",
    "    joined=np.zeros([len(args),int(max(lengths))])\n",
    "    for i in range(len(args)):\n",
    "        joined[i,:int(lengths[i])]=args[i]\n",
    "    return joined\n",
    "def gap_prob(gap,prob,rollover,eps):\n",
    "    # naive gap probabilities\n",
    "    prob_sum=prob*(1-prob)**(gap-1)\n",
    "    while ((prob_sum-(prob*(1-prob)**(gap-1+rollover)))/prob_sum)<(1-eps):\n",
    "        gap=gap+rollover\n",
    "        prob_sum=prob_sum+prob*(1-prob)**(gap-1)\n",
    "    return prob_sum\n",
    "def gap_prob_sig(gap,p_s,p_d,size,num_blocks):\n",
    "    # gap probabilities considering duty cycle and frequency of signals\n",
    "    sig_term = 0\n",
    "    if ((gap-1)%size)<(num_blocks-1):\n",
    "        aold = (gap%size)-1\n",
    "    else:\n",
    "        aold = num_blocks-1 \n",
    "    bold = ((gap-1)%size)-aold\n",
    "    for i in range(num_blocks):\n",
    "        if ((i+gap)%size)<num_blocks:\n",
    "            sig_end = 1\n",
    "        else:\n",
    "            sig_end = 0\n",
    "        sig_term += (1-(p_s+p_d))**aold * (1-p_d)**bold * (p_s+p_d)**sig_end * p_d**(1-sig_end)\n",
    "        if ((gap+i)%size)>=num_blocks:\n",
    "            aold -= 1\n",
    "            bold += 1\n",
    "    dark_term = 0\n",
    "    if ((gap-1)%size)<(size-num_blocks-1):\n",
    "        bold = (gap%size)-1\n",
    "    else:\n",
    "        bold = size-num_blocks-1 \n",
    "    aold = ((gap-1)%size)-bold \n",
    "    for i in range(size-num_blocks):\n",
    "        if ((i+gap)%size)<(size-num_blocks):\n",
    "            dark_end = 1\n",
    "        else:\n",
    "            dark_end = 0\n",
    "        dark_term += (1-(p_s+p_d))**aold * (1-p_d)**bold * p_d**dark_end * (p_s+p_d)**(1-dark_end)\n",
    "        if ((gap+i)%size)>=(size-num_blocks):\n",
    "            aold += 1\n",
    "            bold -= 1\n",
    "    prob_sum = ((1-(p_s+p_d))**num_blocks * (1-p_d)**(size-num_blocks))**(divmod(gap-1,size)[0]) *\\\n",
    "    (((p_s+p_d)*sig_term + p_d*dark_term)/(num_blocks*(p_s+p_d)+(size-num_blocks)*p_d))\n",
    "    return prob_sum\n",
    "def gap_prob_rollover(gap,p_s,p_d,size,num_blocks,rollover,eps):\n",
    "    # adding probability contributions from number equal to the gap modulo rollover\n",
    "    # keep adding them until they are extremely irrelevant\n",
    "    prob_sum_old = gap_prob_sig(gap,p_s,p_d,size,num_blocks)\n",
    "    gap += rollover\n",
    "    prob_sum_new = gap_prob_sig(gap,p_s,p_d,size,num_blocks)\n",
    "    while (prob_sum_new/prob_sum_old)>eps:\n",
    "        prob_sum_old += prob_sum_new\n",
    "        gap=gap+rollover\n",
    "        prob_sum_new = gap_prob_sig(gap,p_s,p_d,size,num_blocks)\n",
    "    return prob_sum_old\n",
    "def likelihood_arrays(gaps,p_poor,p_rich,size,num_blocks,rollover,eps):\n",
    "    # make arrays of the probabilities of each gap being a member of signal or dark regions\n",
    "    # this will save us time later and all we will need to do is index these arrays\n",
    "    g1=np.zeros(len(gaps))\n",
    "    g2=np.zeros(len(gaps))\n",
    "    for i in range(len(gaps)):\n",
    "        g1[i]=np.log(gap_prob(gaps[i],p_poor,rollover,eps))\n",
    "        g2[i]=np.log(gap_prob_rollover(gaps[i],p_rich,p_poor,size,num_blocks,rollover,eps))\n",
    "    return g1,g2\n",
    "def log_likelihood(theta,g1,g2,gaps,p_poor,p_rich,rollover,eps):\n",
    "    # computes a likelihood of the gaps given transition points \n",
    "    if 0<(theta[0])<len(gaps) and 0<(theta[1])<len(gaps):# and (theta[0])<(theta[1]):    \n",
    "        theta1=int(theta[0])\n",
    "        theta2=int(theta[1])\n",
    "        dark1=np.sum(g1[:theta1])\n",
    "        sig=np.sum(g2[theta1:theta2])\n",
    "        dark2=np.sum(g1[theta2:])\n",
    "        return dark1+sig+dark2\n",
    "    else:\n",
    "        return 1\n",
    "def log_prior_gauss(theta,gaps,ndim): # not gauss btw\n",
    "    # uniform prior for the position of the transition points, also enforces the order of the transition points\n",
    "    term=np.zeros(ndim)\n",
    "    #print(theta)\n",
    "    if (theta[0])<(theta[1]):\n",
    "        term_other=0\n",
    "    else:\n",
    "        term_other=-np.inf\n",
    "    for i in range(ndim):\n",
    "        if 0<theta[i]<len(gaps):\n",
    "            term[i] =  0\n",
    "        else:\n",
    "            term[i] = -np.inf\n",
    "    return np.sum(term)+term_other\n",
    "    #return -theta**2 / 50\n",
    "def log_posterior_gauss(theta,g1,g2,gaps,p_poor,p_rich,rollover,eps,ndim):  # I lied about the gauss\n",
    "    # likelihood plus prior equals posterior\n",
    "    return log_prior_gauss(theta,gaps,ndim) + log_likelihood(theta,g1,g2,gaps,p_poor,p_rich,rollover,eps)\n",
    "def search_bins(emcee_trace,Nbins):\n",
    "    # after making the mcmc trace, want to efficiently find the optimal points\n",
    "    a1_new=0\n",
    "    a2_new=0\n",
    "    b1_new=Nbins\n",
    "    b2_new=Nbins\n",
    "    a1_old=0\n",
    "    a2_old=0\n",
    "    b1_old=Nbins\n",
    "    b2_old=Nbins\n",
    "    \n",
    "    while (b1_old-a1_old) > 2:\n",
    "        a1_old=a1_new\n",
    "        a2_old=a2_new\n",
    "        b1_old=b1_new\n",
    "        b2_old=b2_new\n",
    "        Nbins_rt=int(np.ceil(np.sqrt(Nbins)))\n",
    "        if (b1_old-a1_old) < 100:\n",
    "            Nbins_rt=int(np.ceil(b1_old)-np.floor(a1_old)+1)\n",
    "            a1_old=np.floor(a1_old)\n",
    "            a2_old=np.floor(a2_old)\n",
    "            b1_old=np.ceil(b1_old)\n",
    "            b2_old=np.ceil(b2_old)\n",
    "        #print('Nbins_rt = ',Nbins_rt)\n",
    "        #print(type(Nbins_rt+1))\n",
    "        #print(Nbins_rt+1)\n",
    "        mc_hist1=plt.hist(emcee_trace[0],bins=np.linspace(a1_old,b1_old,Nbins_rt+1))\n",
    "        mc_hist2=plt.hist(emcee_trace[1],bins=np.linspace(a2_old,b2_old,Nbins_rt+1))\n",
    "        #print(mc_hist2)\n",
    "        ind1=np.where(mc_hist1[0]==max(mc_hist1[0]))[0][0]\n",
    "        ind2=np.where(mc_hist2[0]==max(mc_hist2[0]))[0][0]\n",
    "        #print('ind2 = ',ind2)\n",
    "        a1_new=ind1*(b1_old-a1_old)/Nbins_rt+a1_old\n",
    "        a2_new=ind2*(b2_old-a2_old)/Nbins_rt+a2_old\n",
    "        b1_new=(ind1+1)*(b1_old-a1_old)/Nbins_rt+a1_old\n",
    "        b2_new=(ind2+1)*(b2_old-a2_old)/Nbins_rt+a2_old\n",
    "        Nbins=Nbins_rt\n",
    "    transition_pt=np.zeros(2)\n",
    "    transition_pt[0]=int(b1_old)+1\n",
    "    transition_pt[1]=int(a2_old)-1\n",
    "    #print(transition_pt)\n",
    "    return transition_pt\n",
    "\n",
    "import emcee\n",
    "print('emcee sampling (version: )', emcee.__version__)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def find_transition(Gaps,p_poor,p_rich,size,num_blocks,rollover,eps):\n",
    "    # use an mcmc to explore and optimize the two parameters (the two transition points)\n",
    "    ndim = 2  # number of parameters in the model\n",
    "    nwalkers = 50  # number of MCMC walkers\n",
    "    nsteps = 2000 # steps per walker\n",
    "    nburn = 1000\n",
    "    \n",
    "    g1,g2=likelihood_arrays(Gaps,p_poor,p_rich,size,num_blocks,rollover,eps)\n",
    "    print(f'{nwalkers} walkers: {nsteps} samples each')\n",
    "    # initialize walkers\n",
    "    starting_guesses = np.random.randn(nwalkers, ndim)*len(Gaps)/2 + len(Gaps)/2\n",
    "    sampler = emcee.EnsembleSampler(nwalkers, ndim, log_posterior_gauss, \\\n",
    "                                    args=[g1,g2,Gaps,p_poor,p_rich,rollover,eps,ndim])\n",
    "    # \"burn-in\" period; save final positions and then reset\n",
    "    pos, prob, state = sampler.run_mcmc(starting_guesses, nburn)\n",
    "    sampler.reset()\n",
    "    # sampling period\n",
    "    sampler.run_mcmc(pos, nsteps)\n",
    "    #%time sampler.run_mcmc(starting_guesses, nsteps)\n",
    "    print(\"done\")\n",
    "    print(\"Mean acceptance fraction: {0:.3f} (in total {1} steps)\"\n",
    "                    .format(np.mean(sampler.acceptance_fraction),nwalkers*nsteps))\n",
    "\n",
    "    # sampler.chain is of shape (nwalkers, nsteps, ndim)\n",
    "    # Let us reshape and all walker chains together\n",
    "    emcee_trace = sampler.chain[:, :, :].reshape(-1, ndim).T\n",
    "\n",
    "    transition_pt = search_bins(emcee_trace,Gaps.shape[0])\n",
    "#    mc_hist1=plt.hist(emcee_trace[0],bins=range(Gaps.shape[0]))\n",
    "#    mc_hist2=plt.hist(emcee_trace[1],bins=range(Gaps.shape[0]))\n",
    "#    transition_pt=np.zeros(2)\n",
    "#    transition_pt[0]=mc_hist1[1][np.where(mc_hist1[0]==max(mc_hist1[0]))][0]+1\n",
    "#    transition_pt[1]=mc_hist2[1][np.where(mc_hist2[0]==max(mc_hist2[0]))][0]-1\n",
    "    print(\"transition points: \",transition_pt)\n",
    "    return transition_pt, emcee_trace\n",
    "def mod_dist(x1,x2,mod):\n",
    "    # finds the shortest difference in modulo\n",
    "    # useful for determining which tag came first when we expect them to be clumped relative to the rollover time\n",
    "    if ((x1-x2)%mod)<((x2-x1)%mod):\n",
    "        return -((x1-x2)%mod)\n",
    "    else:\n",
    "        return (x2-x1)%mod\n",
    "def truncate_poor_region(Data_r,p_poor,p_rich_arr,size,num_blocks,rollover,eps):  # make CondData_r a tuple\n",
    "    # Uses the transition points for each channel to cut out the dark region\n",
    "    # determines which channel had first tag using mod_dist, then fills in zeros to rectify array lengths\n",
    "    # then combines them using Join_columns\n",
    "    i_array=[]\n",
    "    new_array=[]\n",
    "    transition_pt=np.zeros([len(Data_r),2])\n",
    "    for i in range(len(Data_r)):    \n",
    "        #TimeData=CondData_r[i][4:,:]\n",
    "        FullTimeData=Data_r[i][4:,:]\n",
    "        Gaps=Make_Gaps(FullTimeData)\n",
    "        transition_pt[i,:] = find_transition(Gaps,p_poor,p_rich_arr[i],size,num_blocks,rollover,eps)[0]\n",
    "        #del TimeData\n",
    "        #rollover = rollover * size\n",
    "        if i==0:\n",
    "            first_pt=read_time_tag(FullTimeData[:,int(transition_pt[i,0])])\n",
    "        else:\n",
    "            if mod_dist(first_pt,read_time_tag(FullTimeData[:,int(transition_pt[i,0])]),rollover)<0:\n",
    "                first_pt=read_time_tag(FullTimeData[:,int(transition_pt[i,0])])\n",
    "                #print(first_pt)\n",
    "        print(\"first transitions point time tag = \",read_time_tag(FullTimeData[:,int(transition_pt[i,0])]))\n",
    "        \n",
    "        i_array.append(ExpandData_sep(FullTimeData[:,int(transition_pt[i,0]):int(transition_pt[i,1])],i))\n",
    "    for i in range(len(Data_r)):\n",
    "        FullTimeData=Data_r[i][4:,:]\n",
    "        print(rollover)\n",
    "        #print(first_pt)\n",
    "        #print(read_time_tag(FullTimeData[:,int(transition_pt[i,0])]))\n",
    "        print(mod_dist(first_pt,read_time_tag(FullTimeData[:,int(transition_pt[i,0])]),rollover))\n",
    "        new_array.append(np.insert(i_array[i],0,np.zeros(int(mod_dist(first_pt,read_time_tag(FullTimeData[:,int(transition_pt[i,0])]),rollover)))))\n",
    "        print(int(mod_dist(first_pt,read_time_tag(FullTimeData[:,int(transition_pt[i,0])]),rollover)))\n",
    "    return Join_columns(new_array)\n",
    "def sync_n_sift(Ncand,Range,size,num_blocks,mu_rich,mu_dec,d_arr,sort,eta,p_send_arr,\\\n",
    "                method,normalize,sent_name,received_name,length_name):\n",
    "    # loads in the data specified by Range and sRange, performs the synchronization \n",
    "    # uses cruncher to time gate and rectify to Alice's rep rate, then performs sifting, outputting QBER and other stats\n",
    "    sRange = [Range[0]-Ncand,Range[1]+Ncand]\n",
    "    #print(sRange)\n",
    "    Mask = np.ones(Range[1]-Range[0])\n",
    "    Ntries=0\n",
    "    recovered_length = np.load(length_name)\n",
    "    new_sent_data_map = np.memmap(sent_name, dtype='uint8',offset = 9*Range[0], shape=(Range[1]-Range[0],9))\n",
    "    sent_data = np.array(new_sent_data_map.T)\n",
    "    del new_sent_data_map\n",
    "    new_received_data_map = np.memmap(received_name, dtype='uint8',offset = 4*sRange[0], shape=(sRange[1]-sRange[0],4))\n",
    "    received_data = np.array(new_received_data_map.T)\n",
    "    print(np.mean(received_data,1))\n",
    "    del new_received_data_map\n",
    "    sync_point = sync_pt(Range[0],sent_data,received_data,sRange,recovered_length,\\\n",
    "                     mu_rich,mu_dec,d_arr,sort,eta,size,num_blocks,p_send_arr,method,Ntries,Mask,normalize)\n",
    "    print(sync_point[0:2])\n",
    "    del received_data\n",
    "    data_start = Range[0]\n",
    "    new_received_data_map = np.memmap(received_name,dtype='uint8',offset = 4*(data_start+sync_point[0]),shape=(Range[1]-Range[0],4))\n",
    "    received_data = np.array(new_received_data_map.T)\n",
    "    del new_received_data_map\n",
    "    test_crunch, crunch_length = cruncher(sent_data,data_start%size,size,num_blocks,sync_point[2])\n",
    "    stored_crunch = cruncher(received_data,data_start%size,size,num_blocks,sync_point[2])[0]\n",
    "    print(np.mean(stored_crunch,1))\n",
    "    del sent_data\n",
    "    del received_data\n",
    "    event_arr = np.zeros([9,16])\n",
    "    #print(crunch_length,test_crunch.shape,stored_crunch.shape)\n",
    "    for i in range(crunch_length):\n",
    "        event_arr[np.where(test_crunch[:,i]==1)[0][0],int(read_time_tag(stored_crunch[:,i])[0])]+=1\n",
    "    SiftedTest, SiftedStored, sift = DataSifter(test_crunch[:4,:],stored_crunch)\n",
    "    sifted_test_map = np.memmap('sifted_test_mem.npy', dtype='uint8', mode='w+', shape=(SiftedTest.shape[1],4))\n",
    "    sifted_test_map[:] = SiftedTest.T\n",
    "    del sifted_test_map\n",
    "    sifted_stored_map = np.memmap('sifted_stored_mem.npy', dtype='uint8', mode='w+', shape=(SiftedStored.shape[1],4))\n",
    "    sifted_stored_map[:] = SiftedStored.T\n",
    "    print(SiftedTest.shape[1],SiftedStored.shape[1])\n",
    "    del sifted_stored_map\n",
    "    my_ind = [0,2]\n",
    "    LR_good = sum(sum(SiftedStored[my_ind[0]:my_ind[1],:]*SiftedTest[my_ind[0]:my_ind[1],:]))\n",
    "    LR_all = sum(sum(SiftedStored[my_ind[0]:my_ind[1],:]+SiftedTest[my_ind[0]:my_ind[1],:]))/2\n",
    "    print('L/R accepted = ',LR_all)\n",
    "    my_ind = [2,4]\n",
    "    HV_good = sum(sum(SiftedStored[my_ind[0]:my_ind[1],:]*SiftedTest[my_ind[0]:my_ind[1],:]))\n",
    "    HV_all = sum(sum(SiftedStored[my_ind[0]:my_ind[1],:]+SiftedTest[my_ind[0]:my_ind[1],:]))/2\n",
    "    print('H/V accepted = ',HV_all) \n",
    "    return LR_good,LR_all,HV_good,HV_all,crunch_length,sync_point,event_arr\n",
    "def cruncher(arr,first,size,num_blocks,whichpoint):\n",
    "    # time gates num_blocks+1 frames (the +1 determined by \"whichpoint\" had more signal)\n",
    "    # rectifies timing to Alice's rep rate, if there are multiple detections for one pulse, these all get rolled into one\n",
    "    #start = int((first - (size/2-1)+np.floor((num_blocks-1)/2))%size)\n",
    "    start = (size-first-(1-whichpoint))%size\n",
    "    leng = int(divmod(arr.shape[1]-start,size)[0])\n",
    "    first_foot = 0\n",
    "    last_foot = 0\n",
    "    new_arr = np.zeros([arr.shape[0],leng])\n",
    "    not_arr = 1-arr\n",
    "    for i in range(leng):\n",
    "        new_arr[:,i] = 1-np.prod(not_arr[:,(i*size+(start)%size):(i*size+(start+num_blocks+1))],1)\n",
    "    #if start != 0:\n",
    "    if 0:\n",
    "        new_arr=np.reshape(np.insert(new_arr.T,0,1-np.prod(not_arr[:,:start],1)),[new_arr.shape[1]+1,arr.shape[0]]).T\n",
    "        first_foot = 1\n",
    "    if 0:\n",
    "    #if ((arr.shape[1] - start)%size) != 0:\n",
    "        new_arr=np.reshape(np.append(new_arr.T,1-np.prod(not_arr[:,(leng*size+start):],1)),[new_arr.shape[1]+1,arr.shape[0]]).T\n",
    "        last_foot = 1\n",
    "    return new_arr, leng+first_foot+last_foot\n",
    "\n",
    "def LFSR(counts,position):   \n",
    "    # generates a pseudorandom sequence for Alice to send\n",
    "    reg = np.array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1])\n",
    "    bit = np.zeros(counts)\n",
    "    for i in range(counts):\n",
    "        bit[i] = reg[-position]\n",
    "        feedback = reg[0]^reg[-13]\n",
    "        reg[:-1] = reg[1:]\n",
    "        reg[-1] = feedback     \n",
    "    return bit\n",
    "\n",
    "def qber_expected(mu_sig,mu_dec,d_arr,sort,eta,p_send_arr,d=2):\n",
    "    # calculates the expected QBER and accepted sifting fraction for the different inputs using the system characteristics\n",
    "    p_click_mat_sig = port_prob(mu_sig,d_arr,sort,eta)\n",
    "    p_click_mat_dec = port_prob(mu_dec,d_arr,sort,eta)\n",
    "    QBER = np.zeros(2*d)\n",
    "    fracs = np.zeros(2*d)\n",
    "    for i in range(2*d):\n",
    "        half = (i>=d)*d\n",
    "        p_hit_H = (p_send_arr[i]*p_click_mat_sig[i,i]+p_send_arr[i+2*d]*p_click_mat_dec[i,i])\\\n",
    "                /(p_send_arr[i]+p_send_arr[i+2*d])\n",
    "        p_hit_V = (p_send_arr[i]*p_click_mat_sig[i,(i+1)%d+half]\\\n",
    "                    +p_send_arr[i+2*d]*p_click_mat_dec[i,(i+1)%d+half])\\\n",
    "                       /(p_send_arr[i]+p_send_arr[i+2*d])\n",
    "        p_hit_R = (p_send_arr[i]*p_click_mat_sig[i,i-2*half+d]\\\n",
    "                    +p_send_arr[i+2*d]*p_click_mat_dec[i,i-2*half+d])\\\n",
    "                       /(p_send_arr[i]+p_send_arr[i+2*d])\n",
    "        p_hit_L = (p_send_arr[i]*p_click_mat_sig[i,(i+1)%d-half+d]\\\n",
    "                    +p_send_arr[i+2*d]*p_click_mat_dec[i,(i+1)%d-half+d])\\\n",
    "                       /(p_send_arr[i]+p_send_arr[i+2*d])\n",
    "        p_wrong = ((1-p_hit_H)*p_hit_V)\n",
    "        p_right = (p_hit_H*(1-p_hit_V))\n",
    "        QBER[i] = p_wrong/(p_right+p_wrong)\n",
    "        fracs[i] = (p_hit_H*(1-p_hit_V)+p_hit_V*(1-p_hit_H))*(1-p_hit_R)*(1-p_hit_L)\n",
    "    return QBER, fracs\n",
    "\n",
    "size=8\n",
    "num_blocks=1\n",
    "bits=16\n",
    "rollover=2**bits\n",
    "poor_darkrate=10e-4  # this should be the measured dark count rate divided by 100 MHz \n",
    "# here I had to raise it to get the code to run correctly\n",
    "#mu_rich = 0.0851648351648352/num_blocks\n",
    "mu_rich = 0.085/num_blocks  # this is the mean photon number in the signal region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate transmitted data and format\n",
    "\n",
    "n_hits = 10000000\n",
    "A = LFSR(n_hits,4)\n",
    "B = LFSR(n_hits,14)\n",
    "data = np.zeros([9,n_hits*size])\n",
    "#data[8,:] = np.ones(n_hits*size)\n",
    "#L = np.zeros(n_hits*size)\n",
    "#R = np.zeros(n_hits*size)\n",
    "#H = np.zeros(n_hits*size)\n",
    "#V = np.zeros(n_hits*size)\n",
    "for i in range(n_hits):\n",
    "    for j in range(num_blocks):\n",
    "        data[1,i*size+j] = (A[i] == 0 and B[i] == 0)\n",
    "        data[0,i*size+j] = (A[i] == 0 and B[i] == 1)\n",
    "        data[2,i*size+j] = (A[i] == 1 and B[i] == 0)\n",
    "        data[8,i*size+j] = (A[i] == 1 and B[i] == 1)\n",
    "    #data[8,i*size+num_blocks] = 0\n",
    "    #data[8,(i+1)*size-1] = 0\n",
    "#data  = np.array([L,R,H,V,dec,dec,dec,dec,dec])\n",
    "print('There are ' , np.sum(data[0,:]), 'L counts')\n",
    "print('There are ' , np.sum(data[1,:]), 'R counts')\n",
    "print('There are ' , np.sum(data[2,:]), 'H counts')\n",
    "#print('There are ' , n_hits- H.count(1)-L.count(1)-R.count(1), 'Empty counts')\n",
    "del A\n",
    "del B\n",
    "\n",
    "sent_data_map = np.memmap('lfsr_mem.npy', dtype='uint8', mode='w+', shape=(n_hits*size,9))\n",
    "sent_data_map[:] = data.T\n",
    "del data\n",
    "del sent_data_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This block reads in the measured data\n",
    "\n",
    "import sys\n",
    "# import numpy as np\n",
    "import struct\n",
    "import os\n",
    "\n",
    "np.set_printoptions(threshold=sys.maxsize)\n",
    "# Read file into script\n",
    "file = open(\"C:\\\\Users\\\\Roddy\\\\Desktop\\\\Python QKD\\\\QKD_receiver_data\\\\data_table_top_qkd_07_03082020.txt\", \"rb\");\n",
    "# change to appropriate file path\n",
    "######data_table_top_qkd_07_03082020\n",
    "# read from file and make four time arrays\n",
    "t_0 = np.array([], dtype=np.uint16);\n",
    "t_1 = np.array([], dtype=np.uint16);\n",
    "t_2 = np.array([], dtype=np.uint16);\n",
    "t_3 = np.array([], dtype=np.uint16);\n",
    "\n",
    "with file as f:\n",
    "    while True:\n",
    "        buff = f.read(65538);\n",
    "        if not buff:\n",
    "            break\n",
    "        s = np.frombuffer( buff, dtype=np.dtype('>u2') );\n",
    "        detector_id = s[16384*2];\n",
    "        if detector_id == 0:\n",
    "            t_0 = np.append( t_0, s[:16384*2] )\n",
    "        elif detector_id == 1:\n",
    "            t_1 = np.append( t_1, s[:16384*2] )\n",
    "        elif detector_id == 2:\n",
    "            t_2 = np.append( t_2, s[:16384*2] )\n",
    "        elif detector_id == 3:\n",
    "            t_3 = np.append( t_3, s[:16384*2] )\n",
    "file.close()\n",
    "del s\n",
    "del buff\n",
    "\n",
    "\n",
    "full_time_tag_tup=[]\n",
    "full_time_tag_tup.append(make_bin_array(t_0[2:195000],bits))\n",
    "full_time_tag_tup.append(make_bin_array(t_1[2:192000],bits))\n",
    "full_time_tag_tup.append(make_bin_array(t_2[2:355000],bits))\n",
    "full_time_tag_tup.append(make_bin_array(t_3[2:230000],bits))\n",
    "del t_0\n",
    "del t_1\n",
    "del t_2\n",
    "del t_3\n",
    "\n",
    "#2:195000\n",
    "#2:192000\n",
    "#2:355000\n",
    "#2:230000\n",
    "# need to make sure these limits are inside the dark count regions bookending exactly one signal region\n",
    "# since this data was taken with a repeating 10 million bit pattern being sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This block generates fake qkd data\n",
    "def superior_fake_data(mu_sig):\n",
    "    n_hits = 10000000\n",
    "    padding = 1000000\n",
    "    A = LFSR(n_hits,4)\n",
    "    B = LFSR(n_hits,14)\n",
    "    fake_data = np.zeros([9,n_hits*size+2*padding])\n",
    "    fake_data[8,:] = np.ones(n_hits*size+2*padding)\n",
    "    fake_mpn = np.zeros(n_hits*size+2*padding)\n",
    "    drift = 0\n",
    "    print(1)\n",
    "    cnt1=0\n",
    "    cnt2=0\n",
    "    cnt3=0\n",
    "    for i in range(n_hits):\n",
    "        drift = drift + (np.random.uniform(-1,1)*1*10**(-2))*0\n",
    "        for j in range(num_blocks):\n",
    "            #if (i%10000)==0:\n",
    "            #    print(round(drift))\n",
    "            #fake_data[8,i*size+j+round(drift)+padding] = 0\n",
    "            fake_data[1,i*size+j+round(drift)+padding] = (A[i] == 0 and B[i] == 0)\n",
    "            fake_data[0,i*size+j+round(drift)+padding] = (A[i] == 0 and B[i] == 1)\n",
    "            fake_data[2,i*size+j+round(drift)+padding] = (A[i] == 1 and B[i] == 0)\n",
    "            fake_data[8,i*size+j+round(drift)+padding] = (A[i] == 1 and B[i] == 1)\n",
    "            rel_time = (j+1-drift)%size\n",
    "            if rel_time>0 and rel_time<1:\n",
    "                fake_mpn[i*size+j+padding+int(drift//size)*size] += rel_time\n",
    "                cnt1 +=1\n",
    "            if rel_time>=1 and rel_time<num_blocks:\n",
    "                fake_mpn[i*size+j+padding] += 1\n",
    "                cnt2 +=1\n",
    "            if rel_time>=num_blocks and rel_time<(num_blocks+1):\n",
    "                fake_mpn[i*size+j+padding+int((drift+num_blocks)//size)*size] += num_blocks + 1 - rel_time\n",
    "                cnt3 +=1\n",
    "    #print(fake_data[:,1500000:1500020])\n",
    "    print(2)\n",
    "    print(cnt1)\n",
    "    print(cnt2)\n",
    "    print(cnt3)\n",
    "    print(drift)\n",
    "    del A\n",
    "    del B\n",
    "    #print(np.sum(fake_mpn))\n",
    "    sent_fakedata_map = np.memmap('fake_data_mem.npy', dtype='uint8', mode='w+', shape=(n_hits*size+2*padding,9))\n",
    "    sent_fakedata_map[:] = fake_data.T\n",
    "    del fake_data\n",
    "    del sent_fakedata_map\n",
    "    sent_mpn_map = np.memmap('mpn_mem.npy', dtype='float64', mode='w+', shape=(n_hits*size+2*padding))\n",
    "    sent_mpn_map[:] = fake_mpn.T\n",
    "    del fake_mpn\n",
    "    del sent_mpn_map\n",
    "\n",
    "\n",
    "    #mu_sig = mu_rich\n",
    "    mu_dec = 0.4 * mu_sig\n",
    "    sort = np.array([[.5,0,.25,.25],[0,.5,.25,.25],[.25,.25,.5,0],[.25,.25,0,.5]])\n",
    "    #sort=np.array([[.5,.5,.5,.5],[.5,.5,.5,.5],[.25,.25,.5,0],[.25,.25,.5,0]]) #only basis is known\n",
    "    eff_simple=np.array([1,1,1,1])\n",
    "    eta = np.diag(eff_simple)\n",
    "    #d_arr=np.array([4.8953635620075145e-05,4.256005392075098e-05,5.9906981710614146e-05,6.011140133429999e-05])/size\n",
    "    d_arr=poor_darkrate*np.ones(4)\n",
    "    #p_send_arr = num_blocks/(size-2) * np.array([.25,.25,.25,0,0,0,0,0,.25+(size-2)/(num_blocks*(size-2-num_blocks))])\n",
    "    p_send_arr = [.25,.25,.25,0,0,0,0,0,.25]\n",
    "\n",
    "\n",
    "    Range = [0,n_hits*size+2*padding]\n",
    "    Stored = np.zeros((4,Range[1]-Range[0]))\n",
    "    opN=1000000\n",
    "    nn,rem = divmod(Range[1],opN)\n",
    "    #print(nn,rem)\n",
    "    for i in range(nn):\n",
    "        new_fake_data_map = np.memmap('fake_data_mem.npy', dtype='uint8',mode = 'r',offset = (9*i*opN), shape=(opN,9))\n",
    "        fake_data = np.array(new_fake_data_map.T)\n",
    "        new_mpn_map = np.memmap('mpn_mem.npy', dtype='float64',mode = 'r',offset = (8*i*opN), shape=(opN))\n",
    "        mpn_arr = np.array(new_mpn_map.T)\n",
    "        del new_fake_data_map\n",
    "        del new_mpn_map\n",
    "\n",
    "        Stored[:,(i*opN):((i+1)*opN)] = FakeData_R(fake_data,mpn_arr,mu_sig,mu_dec,d_arr,sort,eta)\n",
    "        #print(mpn_arr[0])\n",
    "        del fake_data\n",
    "        del mpn_arr\n",
    "    if rem!=0:\n",
    "        new_fake_data_map = np.memmap('fake_data_mem.npy', dtype='uint8',mode = 'r',offset = 9*nn*opN, shape=(rem,9))\n",
    "        fake_data = np.array(new_fake_data_map.T)\n",
    "        new_mpn_map = np.memmap('mpn_mem.npy', dtype='float64',mode = 'r',offset = 8*nn*opN, shape=(rem))\n",
    "        mpn_arr = np.array(new_mpn_map.T)\n",
    "        del new_fake_data_map\n",
    "        del new_mpn_map\n",
    "\n",
    "        Stored[:,(nn*opN):] = FakeData_R(fake_data,mpn_arr,mu_sig,mu_dec,d_arr,sort,eta)\n",
    "        del fake_data\n",
    "        del mpn_arr\n",
    "    # this array lets us skip packing and unpacking into the same format as the FPGA output\n",
    "    stored_data_map = np.memmap('stored_data_mem.npy', dtype='uint8',mode = 'w+', shape=(n_hits*size,4))\n",
    "    stored_data_map[:] = Stored[:,padding:-padding].T\n",
    "    del stored_data_map\n",
    "    np.save('stored_length',n_hits*size)\n",
    "    t0 = []\n",
    "    t1 = []\n",
    "    t2 = []\n",
    "    t3 = []\n",
    "    #tup = [t0,t1,t2,t3]\n",
    "    for i in range(Stored.shape[1]):\n",
    "        if Stored[0,i] == 1:\n",
    "            t0.append(i%(2**bits))\n",
    "        if Stored[1,i] == 1:\n",
    "            t1.append(i%(2**bits))\n",
    "        if Stored[2,i] == 1:\n",
    "            t2.append(i%(2**bits))\n",
    "        if Stored[3,i] == 1:\n",
    "            t3.append(i%(2**bits))\n",
    "    t0 = np.array(t0,dtype=np.uint64)\n",
    "    t1 = np.array(t1,dtype=np.uint64)\n",
    "    t2 = np.array(t2,dtype=np.uint64)\n",
    "    t3 = np.array(t3,dtype=np.uint64)\n",
    "    fake_time_tag_tup = []\n",
    "    fake_time_tag_tup.append(make_bin_array(t0,bits))\n",
    "    fake_time_tag_tup.append(make_bin_array(t1,bits))\n",
    "    fake_time_tag_tup.append(make_bin_array(t2,bits))\n",
    "    fake_time_tag_tup.append(make_bin_array(t3,bits))\n",
    "    del t0\n",
    "    del t1\n",
    "    del t2\n",
    "    del t3\n",
    "    del Stored\n",
    "    return 1\n",
    "\n",
    "#print(Stored)\n",
    "#received_fakedata_map = np.memmap('received_fake_data_mem.npy', dtype='uint8', mode='w+', shape=(n_hits*size+2*padding,4))\n",
    "#received_fakedata_map[:] = Stored.T\n",
    "\n",
    "\n",
    "#del received_fakedata_map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This block identifies the transition points at the edges of the signal region, and slices the signal data\n",
    "full_time_tag_tup = fake_time_tag_tup  #use fake data\n",
    "#p_poor=1-(1-poor_darkrate)**size\n",
    "p_poor = poor_darkrate\n",
    "#p_rich=1-(.25*np.exp(-mu_rich/(2*num_blocks))+.5*np.exp(-mu_rich/(4*num_blocks))+.25)\n",
    "p_click_mat_sig = port_prob(mu_rich,d_arr,sort,eta)\n",
    "p_click_mat_dec = port_prob(.4*mu_rich,d_arr,sort,eta)\n",
    "p_click_mat = np.zeros([len(p_send_arr),sort.shape[0]])\n",
    "p_click_mat[:sort.shape[0],:] = p_click_mat_sig[:sort.shape[0],:]\n",
    "p_click_mat[sort.shape[0]:,:] = p_click_mat_dec\n",
    "p_rich_arr = nlik_prob(p_click_mat,p_send_arr)\n",
    "#p_rich=1-(.25*np.exp(-mu_rich/(2*num_blocks))+.5*np.exp(-mu_rich/(4*num_blocks))+.25)\n",
    "#p_rich=1-(1-poor_darkrate)**size*(.25*np.exp(-mu_rich/2)+.5*np.exp(-mu_rich/4)+.25)\n",
    "#p_rich=1-(1-poor_darkrate)**size*(.25*np.exp(-mu_rich)+.75)\n",
    "\n",
    "# R,L,H,V\n",
    "eps=10e-4\n",
    "recovered_data=truncate_poor_region(full_time_tag_tup,p_poor,p_rich_arr,size,num_blocks,rollover,eps)\n",
    "recovered_length = recovered_data.shape[1]\n",
    "del full_time_tag_tup\n",
    "received_data_map = np.memmap('received_mem.npy', dtype='uint8', mode='w+', shape=(recovered_data.T.shape))\n",
    "received_data_map[:] = recovered_data.T\n",
    "del recovered_data\n",
    "del received_data_map\n",
    "# Mean acceptance fractions should be higher than .4 I think.  \n",
    "#  The real proof is whether the transition point time tags are close to each other (usually within a few hundred)\n",
    "print(recovered_length)\n",
    "np.save('recovered_length',recovered_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%time \n",
    "# This block performs synchronization in batches and reads out QBERs, etc.\n",
    "#sort = np.array([[.5,0,.25,.25],[0,.5,.25,.25],[.25,.25,.5,0],[.25,.25,0,.5]])\n",
    "sort=np.array([[.25,.25,.25,.25],[.25,.25,.25,.25],[.25,.25,.5,0],[.25,.25,.5,0]]) #only basis is known\n",
    "eff_simple=np.array([1,1,1,1])\n",
    "eta = np.diag(eff_simple)\n",
    "#d_arr=np.array([4.8953635620075145e-05,4.256005392075098e-05,5.9906981710614146e-05,6.011140133429999e-05])/size\n",
    "d_arr=poor_darkrate*np.ones(4)\n",
    "p_send_arr = [.25,.25,.25,0,0,0,0,0,.25]\n",
    "#mu_rich = 0.0851648351648352/num_blocks\n",
    "mu_dec = 0.4 * mu_rich\n",
    "#poor_darkrate=(5)*4.983147923908952e-05\n",
    "#filenames=['lfsr_mem.npy','stored_data_mem.npy','stored_length.npy'] # use simulated data directly\n",
    "filenames=['lfsr_mem.npy','received_mem.npy','recovered_length.npy']\n",
    "method = 'fft'\n",
    "normalize=1\n",
    "start = 1000000\n",
    "nn=500\n",
    "batch = 100000\n",
    "Ncand = 2000\n",
    "QBs = np.zeros([5,nn])\n",
    "rolling_events = np.zeros((9,16))\n",
    "sync_arr = np.zeros(500)\n",
    "for i in range(nn):\n",
    "    Range = ([int(start+batch*i),int(start+batch*(i+1))])\n",
    "    LR_good,LR_all,HV_good,HV_all,crunch_length,sync_point,event_arr \\\n",
    "    = sync_n_sift(Ncand,Range,size,num_blocks,mu_rich/2,mu_dec,d_arr,sort,eta,p_send_arr,method,normalize,\\\n",
    "                  filenames[0],filenames[1],filenames[2])\n",
    "    QBs[:,i] = [LR_good,LR_all,HV_good,HV_all,crunch_length]\n",
    "    print(' ')\n",
    "    rolling_events=rolling_events+event_arr\n",
    "    sync_arr[i] = sync_point[0]\n",
    "\n",
    "QBs_tot = np.sum(QBs,1)\n",
    "print('Accepted fraction = ',(QBs_tot[1]+QBs_tot[3])/QBs_tot[4])\n",
    "print('L/R QBER = ',1-QBs_tot[0]/QBs_tot[1])\n",
    "print('H/V QBER = ',1-QBs_tot[2]/QBs_tot[3])\n",
    "\n",
    "#print(rolling_events)\n",
    "\n",
    "#outcomes_arr = np.zeros([12,6])\n",
    "#cnt = 0\n",
    "#my_metric_map = np.memmap('metric_data_mem.npy', dtype='float64',offset = 0, shape=(4000))\n",
    "#my_metric = np.array(my_metric_map.T)\n",
    "#del my_metric_map\n",
    "\n",
    "#metric_data_map = np.memmap('metric_data_'+str(batch)+'.npy', dtype='float64',mode = 'w+', shape=(my_metric.shape[0]))\n",
    "#metric_data_map[:] = my_metric.T\n",
    "#del metric_data_map\n",
    "\n",
    "drift_data_map = np.memmap('real_drift_data.npy', dtype='float64',mode = 'w+', shape=(500))\n",
    "drift_data_map[:] = sync_arr.T\n",
    "del drift_data_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the clock drift over time as measured by the synchronization algorithm\n",
    "my_drift_map = np.memmap('real_drift_data.npy', dtype='float64',offset = 0, shape=(500))\n",
    "sync_arr = np.array(my_drift_map.T)\n",
    "del my_drift_map\n",
    "\n",
    "fig_true = plt.figure(figsize=(8,12))\n",
    "ax_true = fig_true.add_subplot(2,1,1)\n",
    "ax_true.plot(np.arange(sync_arr.shape[0])*100000*10*10**(-9),sync_arr*1e-2)\n",
    "print((sync_arr[-1]-sync_arr[0])*10/(sync_arr.shape[0]*100000*10))\n",
    "plt.ylabel('relative clock offset ($\\mu s$)')\n",
    "plt.xlabel('lab time (s)')\n",
    "\n",
    "plt.savefig('qkd_clock_drift_fig.png', dpi=None, facecolor='w', edgecolor='w',\n",
    "        orientation='portrait', papertype=None, format=None,\n",
    "        transparent=False, bbox_inches='tight', pad_inches=0.1,\n",
    "        frameon=None, metadata=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%time\n",
    "# This block compares the probabilities to reality\n",
    "# WARNING: 3 hour run time\n",
    "#sort = np.array([[.5,0,.25,.25],[0,.5,.25,.25],[.25,.25,.5,0],[.25,.25,0,.5]])\n",
    "sort=np.array([[.25,.25,.25,.25],[.25,.25,.25,.25],[.25,.25,.5,0],[.25,.25,.5,0]]) #only basis is known\n",
    "eff_simple=np.array([1,1,1,1])\n",
    "eta = np.diag(eff_simple)\n",
    "#d_arr=np.array([4.8953635620075145e-05,4.256005392075098e-05,5.9906981710614146e-05,6.011140133429999e-05])/size\n",
    "d_arr=poor_darkrate*np.ones(4)\n",
    "p_send_arr = [.25,.25,.25,0,0,0,0,0,.25]\n",
    "\n",
    "mu_rich = 0.05\n",
    "mu_dec = 0.4 * mu_rich # these simulations did not include intermediate decoys but they could\n",
    "#poor_darkrate=(5)*4.983147923908952e-05\n",
    "superior_fake_data(mu_rich)\n",
    "filenames=['lfsr_mem.npy','stored_data_mem.npy','stored_length.npy']\n",
    "#filenames=['lfsr_mem.npy','received_mem.npy','recovered_length.npy']\n",
    "method = 'fft'\n",
    "normalize=1\n",
    "start = 20000\n",
    "nn=1\n",
    "batch = 3200000\n",
    "Ncand = 10000\n",
    "QBs = np.zeros([5,nn])\n",
    "for i in range(nn):\n",
    "    Range = ([int(start+batch*i),int(start+batch*(i+1))])\n",
    "    LR_good,LR_all,HV_good,HV_all,crunch_length,sync_point,event_arr \\\n",
    "    = sync_n_sift(Ncand,Range,size,num_blocks,mu_rich,mu_dec,d_arr,sort,eta,p_send_arr,method,normalize,\\\n",
    "                  filenames[0],filenames[1],filenames[2])\n",
    "    \n",
    "ref_sync = sync_point[0]\n",
    "batch = 20\n",
    "N=1000\n",
    "outcomes_arr = np.zeros([150,6])\n",
    "cnt = 0\n",
    "while (batch) < (8*8000000/N):\n",
    "    \n",
    "    Ncand = 2000\n",
    "    probs = np.zeros(N)\n",
    "    points = np.zeros(N)\n",
    "    for i in range(N):\n",
    "        Range = ([int(start+batch*i),int(start+batch*(i+1))])\n",
    "        LR_good,LR_all,HV_good,HV_all,crunch_length,sync_point,event_arr \\\n",
    "        = sync_n_sift(Ncand,Range,size,num_blocks,mu_rich,mu_dec,d_arr,sort,eta,p_send_arr,method,normalize,\\\n",
    "                      filenames[0],filenames[1],filenames[2])\n",
    "        points[i] = sync_point[0]\n",
    "        probs[i] = sync_point[1]\n",
    "    success_rate = np.sum((points==ref_sync))/N\n",
    "    projected_rate = np.sum(probs)/N\n",
    "    outcomes_arr[cnt,0] = success_rate\n",
    "    outcomes_arr[cnt,1] = projected_rate\n",
    "    outcomes_arr[cnt,2] = np.sqrt(success_rate*(1-success_rate)/N)\n",
    "    outcomes_arr[cnt,3] = np.std(probs)/np.sqrt(N)      #np.sqrt(projected_rate*(1-projected_rate)/N)\n",
    "    outcomes_arr[cnt,4] = batch\n",
    "    outcomes_arr[cnt,5] = N\n",
    "    cnt = cnt+1\n",
    "    batch = int(1.5*batch) \n",
    "file_name = 'outcomes_' + str(mu_rich) +'_6_20_21_mem.npy'\n",
    "#print(file_name)\n",
    "outcomes_map = np.memmap(file_name, dtype='float64', mode='w+', shape=(6,cnt))\n",
    "outcomes_map[:] = outcomes_arr[:cnt,:].T\n",
    "del outcomes_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plots probabilities vs. reality as simulated in the previous block\n",
    "cnt=20\n",
    "mu_label = '0.05'\n",
    "file_name = 'outcomes_' + mu_label +'_6_10_21_mem.npy'\n",
    "new_outcomes_map = np.memmap(file_name, dtype='float64',offset = 0, shape=(6,cnt))\n",
    "outcomes_arr = np.array(new_outcomes_map.T)\n",
    "del new_outcomes_map\n",
    "\n",
    "fig_true = plt.figure(figsize=(8,12))\n",
    "ax_true = fig_true.add_subplot(2,1,1)\n",
    "ax_true.errorbar(np.log10(outcomes_arr[:cnt,4]),np.log10(outcomes_arr[:cnt,0]),\\\n",
    "                 yerr=(outcomes_arr[:cnt,2])/((outcomes_arr[:cnt,0])*np.log(10)),ls='-',label='log$_{10}$ f(S$_j$|B$_1$,...,B$_{M+N}$)')\n",
    "ax_true.errorbar(np.log10(outcomes_arr[:cnt,4]),np.log10(outcomes_arr[:cnt,1]),\\\n",
    "                 yerr=(outcomes_arr[:cnt,3])/((outcomes_arr[:cnt,1])*np.log(10)),ls='-.',label='log$_{10}$ p(S$_j$|B$_1$,...,B$_{M+N}$)')\n",
    "\n",
    "ax_true.errorbar(np.log10(outcomes_arr[:cnt,4]),np.log10(1-outcomes_arr[:cnt,0]),\\\n",
    "                 yerr=(outcomes_arr[:cnt,2])/((1-outcomes_arr[:cnt,0])*np.log(10)),ls='--',label='log$_{10}$(1-f(S$_j$|B$_1$,...,B$_{M+N}$))')\n",
    "ax_true.errorbar(np.log10(outcomes_arr[:cnt,4]),np.log10(1-outcomes_arr[:cnt,1]),\\\n",
    "                 yerr=(outcomes_arr[:cnt,3])/((1-outcomes_arr[:cnt,1])*np.log(10)),ls=':',label='log$_{10}$(1-p(S$_j$|B$_1$,...,B$_{M+N}$))')\n",
    "#plt.ylabel('log$_{10}$ p(S$_j$|B$_1$,...,B$_{M+N}$)')\n",
    "plt.xlabel('log$_{10}$N')\n",
    "plt.title('$\\mu = $' + mu_label)\n",
    "\n",
    "\n",
    "mu_label = '1.0'\n",
    "file_name = 'outcomes_' + mu_label +'_6_10_21_mem.npy'\n",
    "new_outcomes_map = np.memmap(file_name, dtype='float64',offset = 0, shape=(6,cnt))\n",
    "outcomes_arr = np.array(new_outcomes_map.T)\n",
    "del new_outcomes_map\n",
    "ax_true2 = fig_true.add_subplot(2,1,2)\n",
    "ax_true2.errorbar(np.log10(outcomes_arr[:cnt,4]),np.log10(outcomes_arr[:cnt,0]),\\\n",
    "                 yerr=(outcomes_arr[:cnt,2])/((outcomes_arr[:cnt,0])*np.log(10)),ls='-',label='log$_{10}$ f(S$_j$|B$_1$,...,B$_{M+N}$)')\n",
    "ax_true2.errorbar(np.log10(outcomes_arr[:cnt,4]),np.log10(outcomes_arr[:cnt,1]),\\\n",
    "                 yerr=(outcomes_arr[:cnt,3])/((outcomes_arr[:cnt,1])*np.log(10)),ls='-.',label='log$_{10}$ p(S$_j$|B$_1$,...,B$_{M+N}$)')\n",
    "\n",
    "ax_true2.errorbar(np.log10(outcomes_arr[:cnt,4]),np.log10(1-outcomes_arr[:cnt,0]),\\\n",
    "                 yerr=(outcomes_arr[:cnt,2])/((1-outcomes_arr[:cnt,0])*np.log(10)),ls='--',label='log$_{10}$(1-f(S$_j$|B$_1$,...,B$_{M+N}$))')\n",
    "ax_true2.errorbar(np.log10(outcomes_arr[:cnt,4]),np.log10(1-outcomes_arr[:cnt,1]),\\\n",
    "                 yerr=(outcomes_arr[:cnt,3])/((1-outcomes_arr[:cnt,1])*np.log(10)),ls=':',label='log$_{10}$(1-p(S$_j$|B$_1$,...,B$_{M+N}$))')\n",
    "\n",
    "#plt.ylabel('log$_{10}$ p(S$_j$|B$_1$,...,B$_{M+N}$)')\n",
    "plt.xlabel('log$_{10}$N')\n",
    "plt.title('$\\mu = $' + mu_label)\n",
    "ax_true.legend()\n",
    "ax_true2.legend(loc=4)\n",
    "ax_true.text(.5,0,'a)')\n",
    "ax_true2.text(.5,0,'b)')\n",
    "ax_true.set_xlim(1.2,6)\n",
    "ax_true2.set_xlim(1.2,6)\n",
    "plt.subplots_adjust(hspace=0.3)\n",
    "print(cnt)\n",
    "\n",
    "plt.savefig('probability_test.png', dpi=None, facecolor='w', edgecolor='w',\n",
    "        orientation='portrait', papertype=None, format=None,\n",
    "        transparent=False, bbox_inches='tight', pad_inches=0.1,\n",
    "        frameon=None, metadata=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plots sync confidence vs. mu using simulated data generated above\n",
    "cnt = 20\n",
    "name_list = ['outcomes_1.0_6_10_21_mem.npy','outcomes_0.5_6_10_21_mem.npy','outcomes_0.1_6_10_21_mem.npy',\\\n",
    "             'outcomes_0.05_6_10_21_mem.npy','outcomes_0.01_6_10_21_mem.npy','outcomes_0.005_6_10_21_mem.npy']\n",
    "style = ['solid',(0, (1, 10)),'dashed','dashdot',(0, (1, 1)),(0, (3, 5, 1, 5, 1, 5))]\n",
    "fig_true = plt.figure(figsize=(12,16))\n",
    "ax_true = fig_true.add_subplot(2,1,1)\n",
    "for i in range(len(name_list)):\n",
    "    \n",
    "    file_name = name_list[i]\n",
    "    new_outcomes_map = np.memmap(file_name, dtype='float64',offset = 0, shape=(6,cnt))\n",
    "    outcomes_arr = np.array(new_outcomes_map.T)\n",
    "    del new_outcomes_map\n",
    "    \n",
    "    ax_true.plot(np.log10(outcomes_arr[:cnt,4]),np.log10(outcomes_arr[:cnt,1]),\\\n",
    "                 ls=style[i],label='$\\mu = $'+file_name[9:-16])\n",
    "plt.ylabel('log$_{10}$ p(S$_j$|B$_1$,...,B$_{M+N}$)')\n",
    "plt.xlabel('log$_{10}$N')\n",
    "plt.title('Average Confidence')\n",
    "#plt.title('$\\mu = $' + mu_label)\n",
    "ax_true.legend()\n",
    "ax_true.set_xlim(1.2,5)\n",
    "plt.savefig('sync_confidence_parameterized.png', dpi=None, facecolor='w', edgecolor='w',\n",
    "        orientation='portrait', papertype=None, format=None,\n",
    "        transparent=False, bbox_inches='tight', pad_inches=0.1,\n",
    "        frameon=None, metadata=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This block finds string length necessary to achieve 95% synchronization confidence\n",
    "# WARNING: multi-hour run time\n",
    "sort=np.array([[.25,.25,.25,.25],[.25,.25,.25,.25],[.25,.25,.5,0],[.25,.25,.5,0]]) #only basis is known\n",
    "eff_simple=np.array([1,1,1,1])\n",
    "eta = np.diag(eff_simple)\n",
    "#d_arr=np.array([4.8953635620075145e-05,4.256005392075098e-05,5.9906981710614146e-05,6.011140133429999e-05])/size\n",
    "d_arr=poor_darkrate*np.ones(4)\n",
    "\n",
    "p_send_arr = [.25,.25,.25,0,0,0,0,0,.25]\n",
    "method = 'fft'\n",
    "normalize=1\n",
    "filenames=['lfsr_mem.npy','stored_data_mem.npy','stored_length.npy']\n",
    "ref_sync = 0\n",
    "N=100\n",
    "start = 20000\n",
    "Ncand = 2000\n",
    "def f_outcomes(batch,mu):\n",
    "    outcomes_arr = np.zeros([6,1])\n",
    "    probs = np.zeros(N)\n",
    "    points = np.zeros(N)\n",
    "    for i in range(N):\n",
    "        Range = ([int(start+batch*i),int(start+batch*(i+1))])\n",
    "        LR_good,LR_all,HV_good,HV_all,crunch_length,sync_point,event_arr \\\n",
    "        = sync_n_sift(Ncand,Range,size,num_blocks,mu,.4*mu,d_arr,sort,eta,p_send_arr,method,normalize,\\\n",
    "                        filenames[0],filenames[1],filenames[2])\n",
    "        points[i] = sync_point[0]\n",
    "        probs[i] = sync_point[1]\n",
    "    success_rate = np.sum((points==ref_sync))/N\n",
    "    projected_rate = np.sum(probs)/N\n",
    "    outcomes_arr[0] = success_rate\n",
    "    outcomes_arr[1] = projected_rate\n",
    "    outcomes_arr[2] = np.sqrt(success_rate*(1-success_rate)/N)\n",
    "    outcomes_arr[3] = np.std(probs)/np.sqrt(N)      #np.sqrt(projected_rate*(1-projected_rate)/N)\n",
    "    outcomes_arr[4] = batch\n",
    "    outcomes_arr[5] = N\n",
    "    return outcomes_arr\n",
    "\n",
    "mus=np.logspace(-2,0,25)\n",
    "outcomes_perm = np.zeros([25,7])\n",
    "for j in range(25):\n",
    "    superior_fake_data(mus[j])\n",
    "    cnt = 0\n",
    "    #batch = 20\n",
    "    projected_rate = 0\n",
    "    f_a = f_outcomes(100,mus[j])\n",
    "    f_b = f_outcomes(int(8*8000000/N),mus[j])\n",
    "    a = 100\n",
    "    b = int(8*8000000/N)\n",
    "    stop = 0\n",
    "    while (stop==0):\n",
    "        if f_b[1]<.95:\n",
    "            outcomes_arr = f_b\n",
    "            stop = 1\n",
    "        \n",
    "        c = int((a+b)/2)        \n",
    "        if ((c == a) or (c == b)):\n",
    "            outcomes_arr = f_b\n",
    "            stop = 1\n",
    "        f_c = f_outcomes(c,mus[j])\n",
    "        if f_c[1]<.95:\n",
    "            a = c\n",
    "            f_a = f_c\n",
    "        else:\n",
    "            b = c\n",
    "            f_b = f_c\n",
    "        \n",
    "        cnt = cnt+1\n",
    "        #batch = int(1.5*batch) \n",
    "    outcomes_perm[j,:-1] = outcomes_arr[:,0]\n",
    "    outcomes_perm[j,-1] = mus[j]\n",
    "file_name = 'synclength_vs_confidence_5_08_21_mem.npy'\n",
    "#print(file_name)\n",
    "outcomes_map = np.memmap(file_name, dtype='float64', mode='w+', shape=(7,25))\n",
    "outcomes_map[:] = outcomes_perm.T\n",
    "del outcomes_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plots string length necessary to achieve 95% synchronization confidence vs. mu as simulated in previous block\n",
    "new_outcomes_map = np.memmap('synclength_vs_confidence_4_30_21_mem.npy', dtype='float64',offset = 0, shape=(7,25))\n",
    "outcomes_perm = np.array(new_outcomes_map.T)\n",
    "del new_outcomes_map\n",
    "\n",
    "fig_true = plt.figure(figsize=(8,12))\n",
    "ax_true = fig_true.add_subplot(2,1,1)\n",
    "ax_true.plot(np.log10(outcomes_perm[:,6]),np.log10(outcomes_perm[:,4]),'-',label=r'd = 8 $\\times$ 10$^{-4}$')\n",
    "plt.ylabel('log$_{10}$N')\n",
    "plt.xlabel('log$_{10}(\\mu)$')\n",
    "plt.title('Threshold for 95% confidence')\n",
    "print(outcomes_perm[:,4])\n",
    "\n",
    "new_outcomes_map = np.memmap('synclength_vs_confidence_5_08_21_mem.npy', dtype='float64',offset = 0, shape=(7,25))\n",
    "outcomes_perm = np.array(new_outcomes_map.T)\n",
    "del new_outcomes_map\n",
    "ax_true.plot(np.log10(outcomes_perm[:,6]),np.log10(outcomes_perm[:,4]),'--',label=r'd = 8 $\\times$ 10$^{-3}$')\n",
    "\n",
    "new_outcomes_map = np.memmap('synclength_vs_confidence_5_06_21_mem.npy', dtype='float64',offset = 0, shape=(7,25))\n",
    "outcomes_perm = np.array(new_outcomes_map.T)\n",
    "del new_outcomes_map\n",
    "ax_true.plot(np.log10(outcomes_perm[:,6]),np.log10(outcomes_perm[:,4]),':',label=r'd = 8 $\\times$ 10$^{-2}$')\n",
    "\n",
    "ax_true.legend()\n",
    "\n",
    "plt.savefig('sync_confidence_95.png', dpi=None, facecolor='w', edgecolor='w',\n",
    "        orientation='portrait', papertype=None, format=None,\n",
    "        transparent=False, bbox_inches='tight', pad_inches=0.1,\n",
    "        frameon=None, metadata=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
